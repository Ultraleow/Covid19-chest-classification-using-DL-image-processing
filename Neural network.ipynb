{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AKaMCRbNpaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7ecc7a8-bb59-40fd-c0a3-70a4b11818ca"
      },
      "source": [
        "!pip install alexnet_pytorch\r\n",
        "from alexnet_pytorch import AlexNet\r\n",
        "import json\r\n",
        "import torch\r\n",
        "import torchvision.transforms as transforms\r\n",
        "from PIL import Image\r\n",
        "import torchvision.datasets as datasets\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import time\r\n",
        "# Run this cell to mount your Google Drive.\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "import sys\r\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/image_pro')\r\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/image_pro"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: alexnet_pytorch in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from alexnet_pytorch) (1.7.0+cu101)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->alexnet_pytorch) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->alexnet_pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->alexnet_pytorch) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->alexnet_pytorch) (0.16.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks/image_pro\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrBlWxIzHmy6",
        "outputId": "94d8d9cd-a4b9-4528-e27d-2452cc0f6f5a"
      },
      "source": [
        "if torch.cuda.is_available():\r\n",
        "  print(3)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryrPybneN5r1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99a7f97f-18ef-4860-e5c6-59d4139a5654"
      },
      "source": [
        "# define model parameters\r\n",
        "NUM_EPOCHS = 20 \r\n",
        "BATCH_SIZE = 64\r\n",
        "MOMENTUM = 0.9\r\n",
        "LR_DECAY = 0.0005\r\n",
        "LR_INIT = 0.01\r\n",
        "IMAGE_DIM = 227  # pixels\r\n",
        "NUM_CLASSES = 2  #number of bit\r\n",
        "DEVICE_IDS = [0, 1, 2,3]  # GPUs to use\r\n",
        "#DEVICE_IDS = [0]\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "model = AlexNet.from_pretrained('alexnet',num_classes=2).to(device)\r\n",
        "model.eval()\r\n",
        "\r\n",
        "# modify this to point to your data directory\r\n",
        "ROOT = r'/content/drive/MyDrive/Colab Notebooks/image_pro/'\r\n",
        "#INPUT_ROOT_DIR = 'alexnet_data_in'\r\n",
        "TRAIN_IMG_DIR = ROOT+'datasets_post/train'\r\n",
        "TEST_IMG_DIR = ROOT+'datasets_post/test'\r\n",
        "VALI_IMG_DIR = ROOT+'datasets_post/vali'\r\n",
        "OUTPUT_DIR = ROOT+'result'\r\n",
        "\r\n",
        "LOG_DIR = OUTPUT_DIR + '/tblogs'  # tensorboard logs\r\n",
        "CHECKPOINT_DIR = OUTPUT_DIR + '/models'  # model checkpoints"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for alexnet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFMDnO_sN683",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c548083-3dbc-42ad-8c45-ca82958b3f97"
      },
      "source": [
        "# print the seed value\r\n",
        "seed = torch.initial_seed()\r\n",
        "print('Used seed : {}'.format(seed))\r\n",
        "#tbwriter = SummaryWriter(log_dir=LOG_DIR)\r\n",
        "#print('TensorboardX summary writer created')\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Used seed : 15126668862055365485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdbaji35OJ-T"
      },
      "source": [
        "means=[0.485, 0.456, 0.406]\r\n",
        "stds=[0.229, 0.224, 0.225]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV-hT9JqOAav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "423e5832-5e2a-495a-e1b5-1be36ad8f0ce"
      },
      "source": [
        "ROOT = '.data'\r\n",
        "train_data = datasets.CIFAR10(root = ROOT, \r\n",
        "                             train = True, \r\n",
        "                             download = True)\r\n",
        "\r\n",
        "means = train_data.data.mean(axis = (0,1,2)) / 255\r\n",
        "stds = train_data.data.std(axis = (0,1,2)) / 255"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "585odnS7OMkF"
      },
      "source": [
        "train_transforms = transforms.Compose([\r\n",
        "                                       transforms.Resize(800),\r\n",
        "                           transforms.RandomRotation(5),\r\n",
        "                           transforms.RandomHorizontalFlip(0.5),\r\n",
        "                          #  transforms.RandomCrop(32, padding = 2),\r\n",
        "                           transforms.ToTensor(),\r\n",
        "                           transforms.Normalize(mean = means, \r\n",
        "                                                std = stds)\r\n",
        "                       ])\r\n",
        "test_transforms = transforms.Compose([\r\n",
        "                                      transforms.Resize(800),\r\n",
        "                           transforms.ToTensor(),\r\n",
        "                           transforms.Normalize(mean = means, \r\n",
        "                                                std = stds)\r\n",
        "                       ])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im_3K-yIOQ5W"
      },
      "source": [
        "def fetchData():\r\n",
        "    '''\r\n",
        "    fetch data from file.\r\n",
        "    @return: train, test and valid dataset\r\n",
        "    '''\r\n",
        "    train_path = TRAIN_IMG_DIR # edit me\r\n",
        "    valid_path = VALI_IMG_DIR # edit me\r\n",
        "    test_path = TEST_IMG_DIR\r\n",
        "    train_data = datasets.ImageFolder(train_path, transform=train_transforms)\r\n",
        "    valid_data = datasets.ImageFolder(valid_path, transform=test_transforms)\r\n",
        "    test_data = datasets.ImageFolder(test_path, transform=test_transforms)\r\n",
        "    return train_data, valid_data, test_data\r\n",
        "    #return train_data,test_path\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "train_data,valid_data,test_data = fetchData()\r\n",
        "params = {'batch_size': BATCH_SIZE,\r\n",
        "          'shuffle': True,\r\n",
        "          'num_workers': len(DEVICE_IDS)}\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(train_data, **params)\r\n",
        "vali_loader = torch.utils.data.DataLoader(valid_data,**params)\r\n",
        "test_loader = torch.utils.data.DataLoader(test_data,**params)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjVSNP_jOVjE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa2b23b6-478d-4adc-aee5-2a0408017a1e"
      },
      "source": [
        "optimizer = optim.Adam(params=model.parameters(), lr=0.0001, weight_decay=5e-2)\r\n",
        "# multiply LR by 1 / 10 after every 30 epochs\r\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\r\n",
        "print('LR Scheduler created')\r\n",
        "# start training!!\r\n",
        "print('Starting training...')\r\n",
        "total_steps = 1\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "criterion = criterion.to(device)\r\n",
        "\r\n",
        "def save_model(model, filename):\r\n",
        "    filename = os.path.join(\"checkpoints\", filename + \".pth\")\r\n",
        "    dirname = os.path.dirname(filename)\r\n",
        "    os.makedirs(dirname, exist_ok=True)\r\n",
        "    torch.save(model.state_dict(), filename)\r\n",
        "def calculate_accuracy(y_pred, y):\r\n",
        "    top_pred = y_pred.argmax(1, keepdim = True)\r\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\r\n",
        "    acc = correct.float() / y.shape[0]\r\n",
        "    return acc\r\n",
        "\r\n",
        "def train(model, iterator, optimizer, criterion, device):\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    for (x, y) in iterator:\r\n",
        "        \r\n",
        "        x = x.to(device)\r\n",
        "        y = y.to(device)\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "                \r\n",
        "        y_pred = model(x)\r\n",
        "        \r\n",
        "        loss = criterion(y_pred, y)\r\n",
        "        \r\n",
        "        acc = calculate_accuracy(y_pred, y)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_acc += acc.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\r\n",
        "\r\n",
        "def evaluate(model, iterator, criterion, device):\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "        \r\n",
        "        for (x, y) in iterator:\r\n",
        "\r\n",
        "            x = x.to(device)\r\n",
        "            y = y.to(device)\r\n",
        "\r\n",
        "            y_pred = model(x)\r\n",
        "\r\n",
        "            loss = criterion(y_pred, y)\r\n",
        "\r\n",
        "            acc = calculate_accuracy(y_pred, y)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "            epoch_acc += acc.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\r\n",
        "\r\n",
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs\r\n",
        "\r\n",
        "best_valid_loss = float('inf')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR Scheduler created\n",
            "Starting training...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P45FKMsgOXVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10185c98-2203-4296-af46-4cd2e0cba70d"
      },
      "source": [
        "for epoch in range(NUM_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.monotonic()\r\n",
        "    \r\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\r\n",
        "    valid_loss, valid_acc = evaluate(model, vali_loader, criterion, device)\r\n",
        "        \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut3-model.pt')\r\n",
        "\r\n",
        "    end_time = time.monotonic()\r\n",
        "\r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.424 | Train Acc: 80.63%\n",
            "\t Val. Loss: 0.905 |  Val. Acc: 50.19%\n",
            "Epoch: 02 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.261 | Train Acc: 90.15%\n",
            "\t Val. Loss: 1.380 |  Val. Acc: 51.71%\n",
            "Epoch: 03 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.229 | Train Acc: 90.66%\n",
            "\t Val. Loss: 1.525 |  Val. Acc: 50.58%\n",
            "Epoch: 04 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.201 | Train Acc: 93.10%\n",
            "\t Val. Loss: 1.145 |  Val. Acc: 51.91%\n",
            "Epoch: 05 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.178 | Train Acc: 93.53%\n",
            "\t Val. Loss: 1.359 |  Val. Acc: 52.16%\n",
            "Epoch: 06 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.174 | Train Acc: 93.89%\n",
            "\t Val. Loss: 1.771 |  Val. Acc: 50.31%\n",
            "Epoch: 07 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.176 | Train Acc: 93.50%\n",
            "\t Val. Loss: 1.741 |  Val. Acc: 52.32%\n",
            "Epoch: 08 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.167 | Train Acc: 93.28%\n",
            "\t Val. Loss: 2.262 |  Val. Acc: 50.35%\n",
            "Epoch: 09 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.171 | Train Acc: 93.92%\n",
            "\t Val. Loss: 1.383 |  Val. Acc: 51.52%\n",
            "Epoch: 10 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.171 | Train Acc: 93.17%\n",
            "\t Val. Loss: 2.094 |  Val. Acc: 49.92%\n",
            "Epoch: 11 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.173 | Train Acc: 92.93%\n",
            "\t Val. Loss: 1.163 |  Val. Acc: 54.34%\n",
            "Epoch: 12 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.153 | Train Acc: 94.60%\n",
            "\t Val. Loss: 1.574 |  Val. Acc: 50.78%\n",
            "Epoch: 13 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.140 | Train Acc: 94.24%\n",
            "\t Val. Loss: 1.964 |  Val. Acc: 51.21%\n",
            "Epoch: 14 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.139 | Train Acc: 94.44%\n",
            "\t Val. Loss: 1.571 |  Val. Acc: 53.25%\n",
            "Epoch: 15 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.143 | Train Acc: 94.67%\n",
            "\t Val. Loss: 1.664 |  Val. Acc: 54.77%\n",
            "Epoch: 16 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.130 | Train Acc: 94.55%\n",
            "\t Val. Loss: 1.905 |  Val. Acc: 51.17%\n",
            "Epoch: 17 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.110 | Train Acc: 96.20%\n",
            "\t Val. Loss: 1.627 |  Val. Acc: 54.18%\n",
            "Epoch: 18 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.158 | Train Acc: 92.88%\n",
            "\t Val. Loss: 2.335 |  Val. Acc: 50.31%\n",
            "Epoch: 19 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.139 | Train Acc: 94.89%\n",
            "\t Val. Loss: 2.010 |  Val. Acc: 49.49%\n",
            "Epoch: 20 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.115 | Train Acc: 96.09%\n",
            "\t Val. Loss: 2.094 |  Val. Acc: 52.16%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqayz_NTBv6i"
      },
      "source": [
        "torch.save(model,'/content/drive/MyDrive/Colab Notebooks/image_pro/test_2.pt')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr2jatupBBGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691b8d1c-c6f8-4d7c-a583-4287e12031f7"
      },
      "source": [
        "ok=torch.load('test_2.pt')\r\n",
        "#model.load_state_dict(ok)\r\n",
        "\r\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\r\n",
        "\r\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.319 | Test Acc: 90.18%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qsUaNLQBCdI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "ac5d8ff9-4bfe-4ed9-ff36-5402297d1868"
      },
      "source": [
        "import torch.nn.functional as F\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\r\n",
        "def get_predictions(model, iterator, device):\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    images = []\r\n",
        "    labels = []\r\n",
        "    probs = []\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "\r\n",
        "        for (x, y) in iterator:\r\n",
        "\r\n",
        "            x = x.to(device)\r\n",
        "\r\n",
        "            y_pred = model(x)\r\n",
        "\r\n",
        "            y_prob = F.softmax(y_pred, dim = -1)\r\n",
        "            top_pred = y_prob.argmax(1, keepdim = True)\r\n",
        "\r\n",
        "            # images.append(x.cpu())\r\n",
        "            labels.append(y.cpu())\r\n",
        "            probs.append(y_prob.cpu())\r\n",
        "\r\n",
        "    #images = torch.cat(images, dim = 0)\r\n",
        "    labels = torch.cat(labels, dim = 0)\r\n",
        "    probs = torch.cat(probs, dim = 0)\r\n",
        "\r\n",
        "    return images, labels, probs\r\n",
        "images, labels, probs = get_predictions(model, test_loader, device)\r\n",
        "pred_labels = torch.argmax(probs, 1)\r\n",
        "def plot_confusion_matrix(labels, pred_labels, classes):\r\n",
        "    \r\n",
        "    fig = plt.figure(figsize = (10, 10));\r\n",
        "    ax = fig.add_subplot(1, 1, 1);\r\n",
        "    cm = confusion_matrix(labels, pred_labels);\r\n",
        "    cm = ConfusionMatrixDisplay(cm, display_labels = classes);\r\n",
        "    cm.plot(values_format = 'd', cmap = 'Blues', ax = ax)\r\n",
        "    plt.xticks(rotation = 20)\r\n",
        "plot_confusion_matrix(labels, pred_labels, ['Positive','Negative'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl8AAAItCAYAAAAHVR/XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dedxuc73/8dd7b/MYYYdMlSGVuSgNpIHqHClp8DtRneSE5vkMokNK0iB1lIpUhqg4OSQNyknZZJY4IWTaZpGwP78/1rq57LP37d77uNa63dfr6XE97vta17XW+l6be/vc7+9nfVeqCkmSJHVjWt8DkCRJGiUWX5IkSR2y+JIkSeqQxZckSVKHLL4kSZI6tFDfA5AkSaNt+jJrVD1wb2fnq3tvPrWqtu3shHOw+JIkSb2qB+5l0XV36ux8fz3vSyt0drK5cNpRkiSpQyZfkiSpZ4GMTh40Op9UkiRpEjD5kiRJ/QqQ9D2Kzph8SZIkdcjkS5Ik9c+eL0mSJA2DyZckSeqfPV+SJEkaBpMvSZLUM9f5kiRJ0pCYfEmSpP7Z8yVJkqRhsPiSJEnqkNOOkiSpX8GGe0mSJA2HyZckSepZbLiXJEnScJh8SZKk/tnzJUmSpGEw+ZIkSf2z50uSJEnDYPIlSZJ65o21JUmSNCQmX5IkqV/Bni9JkiQNh8mXJEnqnz1fkiRJGgaTL0mS1DOvdpQkSdKQWHxJkiR1yGlHSZLUv2kuNSFJkqQhMPmSJEn9CjbcS5IkaThMviRJUv+8vZAkSZKGweRLkiT1zEVWJUmSNCQmX5IkqX/2fEmSJGkYTL4kSVL/7PmSJEnSMJh8SZKkfiX2fEmSJGk4TL4kSVL/7PmSJEnSMJh8TUAWWryyyNJ9D0MaORs/ffW+hyCNpKuvvopZs2aNThNWxyy+JiCLLM2i6+7U9zCkkXPmbw7pewjSSNpy8826P6kN95IkSRoGky9JktQzb6wtSZKkITH5kiRJ/bPnS5IkScNg8SVJkvoVmp6vrh7jDSVZLcnPklyS5OIk7263fzzJdUnOax+vGNjno0muSHJZkpc/2sd12lGSJOlhDwDvr6pzkywNnJPktPa1g6vqM4NvTrI+8AbgGcAqwE+SrFNVD87rBBZfkiSpZ5Pnasequh64vv3+riSXAquOs8v2wNFVdR9wZZIrgOcAv57XDpPjk0qSJHVnhSQzBx67ze1NSdYENgZ+027aM8kFSb6eZLl226rANQO7Xcv4xZrJlyRJmgS6vdpxVlWNu4x/kqWA44H3VNWdSb4MfAKo9utBwFsX5OQmX5IkSQOSLExTeH27qk4AqKobq+rBqpoNfJVmahHgOmC1gd2f3G6bJ4svSZLUv8lztWOAw4FLq+qzA9tXHnjbDsBF7fcnAm9IsmiStYC1gd+Odw6nHSVJkh62JfAPwIVJzmu3fQx4Y5KNaKYdrwLeAVBVFyc5FriE5krJPca70hEsviRJ0mQwSVa4r6pf0aw8NqeTx9lnP2C/iZ7DaUdJkqQOWXxJkiR1yGlHSZLUr0yeRVa7MDqfVJIkaRIw+ZIkSf2bJA33XTD5kiRJ6pDJlyRJ6l1MviRJkjQMJl+SJKlXweRLkiRJQ2LyJUmS+hXmfkOfKcrkS5IkqUMmX5IkqWex50uSJEnDYfIlSZJ6Z/IlSZKkoTD5kiRJvTP5kiRJ0lBYfEmSJHXIaUdJktQ7px0lSZI0FCZfkiSpX95eSJIkScNi8iVJknoVby8kSZKkYTH5kiRJvTP5kiRJ0lCYfEmSpN6ZfEmSJGkoTL4kSVLvTL4kSZI0FCZfkiSpX65wL0mSpGEx+ZIkSb2z50uSJElDYfIlSZJ65b0dJUmSNDQWX5IkSR1y2lGSJPXOaUdJkiQNhcmXJEnq3+gEXyZfkiRJXTL5kiRJ/Yo9X5IkSRoSky9JktQ7ky9JkiQNhcmXJEnqncmXJEmShsLkS5Ik9coba0uSJGloTL4kSVL/Rif4MvmSJEnqksmXJEnqlyvcS5IkaVgsviRJkjrktKMkSeqd046SJEkaCpMvSZLUO5MvSZIkDYXJlyRJ6t/oBF8mX5IkSV0y+ZIkSb2z50uSJElDYfIlSZJ6lcTkS5IkScNh8iVJknpn8iVJkqShMPmSJEm9M/mSJEnSUJh8SZKk/o1O8GXyJUmS1CWLL0mSpA457ShJknpnw70kSZKGwuRLkiT1KyZfkiRJGhKTL0mS1KsAIxR8mXxJkiR1yeRLkiT1LPZ8SZIkaThMviRJUu9GKPgy+ZIkSeqSyZckSeqdPV+SJEkaCpMvSZLUr9jzJUmSpCEx+ZIkSb0KMG3a6ERfJl+SJEkdsviSJEnqkNOOkiSpdzbcS5IkaShMvvS4tuqMJ/Dlj7+ZFZdfmgKO+P6Z/MfRP+fw/d/C2mvMAGDZpRbnjrvv5YU7H8DCC03n4I+9kY2fvjqzZ8/mIwcdz5nnXt7vh5CmgD33PYpTf3URKyy3NL8+5p8B+MFPzuVTh53MZVfdyOnf/AAbr79Gz6PUZDZKi6z2UnwleRC4sD3/pcAuVXXPfOy/CvCFqtoxyUbAKlV1cvva3wPrV9UBQxi6JpkHHpjNv3zuBC647FqWWmJRfnbkh/n5b37P2z72jYfe84n37MCdd98LwC47bAnAlm/cnxWWW4rjPv9OXrzLgVRVL+OXpoo3vmoL3r7Ti9h97yMf2vb0p67CkZ9+O+/95Hd7HJk0f5KsBhwJzAAKOKyqPp9keeAYYE3gKmCnqrotTdX4eeAVwD3ArlV17njn6Gva8d6q2qiqngn8Ddh9fnauqj9X1Y7t041oPvDYaydaeI2OG2+5kwsuuxaAu++5jz9cdQMrr/iER7xnh5dswvGnngPAums9iV+efRkAs267mzvuvpeNn756t4OWpqAtN3kayy2zxCO2rbvWk1h7zRk9jUiPK+0iq109HsUDwPuran1gC2CPJOsDHwFOr6q1gdPb5wDbAWu3j92ALz/aCSZDz9cvgaclWT7JD5JckOSsJBsAJHlRkvPax++SLJ1kzSQXJVkE2Bd4ffv665PsmuSQJMsmuTrJtPY4Sya5JsnCSZ6a5JQk5yT5ZZL1evz8eoystvLybLDukznn4qse2va8jZ/KTbfcxR+vuRmAiy6/jm1f+CymT5/G6qs8kY3WW41VZyzX04glSZNNVV0/llxV1V00M3SrAtsDR7RvOwJ4dfv99sCR1TgLeEKSlcc7R689X0kWoqkYTwH2AX5XVa9O8mKayG8j4APAHlV1ZpKlgL+O7V9Vf0vyb8BmVbVne8xd29fuSHIe8CLgZ8CrgFOr6v4khwG7V9XlSTYHDgVePMfYdqOpYGHhpYb1R6DHyJKLL8KRn/pHPvrZ47nrLw/9J8JrX7YZx/945kPPjzrx16yz5gx+duSHuOb6W/ntBVfy4OzZfQxZktQKnfd8rZBk5sDzw6rqsDnflGRNYGPgN8CMqrq+fekGmmlJaAqzawZ2u7bddj3z0FfxtXhbGEGTfB1O88FeC1BVP03yxCTLAGcCn03ybeCEqrp2Pv4FHQO8nqb4egNwaFvAPQ84buA4i865Y/sv4TCAaUusZEPQJLbQ9Gkc8am3c9wpM/nPn53/0Pbp06fxqq03ZOs3f/qhbQ8+OJt/PviEh56fevj7+J8/3dTpeCVJvZtVVZuN94a2XjgeeE9V3TlYe1RVJVng2qCv4uveqtpocMO8CqqqOiDJj2j6us5M8nIG0q9HcSKwf9sktynwU2BJ4PY5z6/Hry/+68784aobOPQ7P33E9q2esy6XX30jf77p9oe2Lb7owiThnr/+ja2esx4PPDCby668oeshS5IeIZPqasckC9MUXt+uqrHf2G9MsnJVXd9OK4795n4dsNrA7k9ut83TZFpq4pfAzsAnkmxFU5XemeSpVXUhcGGSZwPrAecN7HcXsPTcDlhVdyc5m+YqhP+sqgeBO5NcmeR1VXVce5XCBlV1/tyOocltiw2fwhteuTkXX34dZ3y76X38xJdO5LT/voTXvGzThxrtx6yw/NIc/8U9mD27uP7m29l97yPmdlhJ8+lt//wNzjzncm65/W6e8cp/4SO7vYLlllmSD3/mOGbddjevf+9XeNY6q3L8F/fse6jSuNq64HDg0qr67MBLJwK7AAe0X384sH3PJEcDmwN3DExPzv0cfVxin+Tuqlpqjm3LA18HnkJzqeZuVXVBki8CWwOzgYuBXYGVaYqpZ7b7nQosDHwSWJxH9oDtCBwHbFVVv2i3rUVzNcLK7X5HV9W+8xrvtCVWqkXX3emx+viSJui2sw/pewjSSNpy880455yZnUVRS6yybq2z26FdnY7z93nJOfOadkzyfJpA6EKa2gPgYzTtUccCqwNX0yw1cWtbrB0CbEtTv7ylqmb+rwMP6CX5mrPwarfdysNXDgxu32suh7gKeObAfs+e4/VvDuz/PZpevsFjXknzhyRJkvSQqvoVc9QNA7aZy/sL2GN+zjGZph0lSdKImkw9X8M2Gdb5kiRJGhkmX5IkqV8TW3l+yjD5kiRJ6pDFlyRJUoecdpQkSb3q4fZCvTL5kiRJ6pDJlyRJ6t0IBV8mX5IkSV0y+ZIkSb2z50uSJElDYfIlSZJ6N0LBl8mXJElSl0y+JElSv2LPlyRJkobE5EuSJPWqWeG+71F0x+RLkiSpQyZfkiSpZ7HnS5IkScNh8iVJkno3QsGXyZckSVKXLL4kSZI65LSjJEnqnQ33kiRJGgqTL0mS1K/YcC9JkqQhMfmSJEm9am4vNDrRl8mXJElSh0y+JElS70y+JEmSNBQmX5IkqXcjFHyZfEmSJHXJ5EuSJPXOni9JkiQNhcmXJEnqlyvcS5IkaVhMviRJUq9C7PmSJEnScFh8SZIkdchpR0mS1LsRmnU0+ZIkSeqSyZckSerdtBGKvky+JEmSOmTyJUmSejdCwZfJlyRJUpdMviRJUq8Sb6wtSZKkITH5kiRJvZs2OsGXyZckSVKXTL4kSVLv7PmSJEnSUJh8SZKk3o1Q8GXyJUmS1CWTL0mS1KsAYXSiL5MvSZKkDll8SZIkdchpR0mS1DsXWZUkSdJQmHxJkqR+JS6yKkmSpOEw+ZIkSb0boeDL5EuSJKlLJl+SJKlXAaaNUPRl8iVJktQhky9JktS7EQq+TL4kSZK6ZPIlSZJ65zpfkiRJGgqTL0mS1KvEni9JkiQNicmXJEnqnet8SZIkaSgsviRJkjrktKMkSerd6Ew6mnxJkiR1yuRLkiT1zkVWJUmSNBQmX5IkqVcBpo1O8GXyJUmS1CWTL0mS1K/Eni9JkiQNh8mXJEnq3QgFXyZfkiRJXTL5kiRJvRulnq95Fl9JvgjUvF6vqncNZUSSJElT2HjJ18zORiFJkkbWqK3zNc/iq6qOGHyeZImqumf4Q5IkSZq6HrXhPslzk1wC/L59vmGSQ4c+MkmSNDLSrvXVxaNvE7na8XPAy4FbAKrqfOCFwxyUJEnSVDWhpSaq6po5Nj04hLFIkiRNeRNZauKaJM8DKsnCwLuBS4c7LEmSNEr6nwzszkSSr92BPYBVgT8DG7XPJUmSNJ8eNfmqqlnAzh2MRZIkjaAEpk2CRviuTORqx6ckOSnJzUluSvLDJE/pYnCSJEldS/L1tua5aGDbx5Ncl+S89vGKgdc+muSKJJclefmjHX8i047fAY4FVgZWAY4Dvjv/H0WSJGnuku4eE/BNYNu5bD+4qjZqHyc34876wBuAZ7T7HJpk+ngHn0jxtURVfauqHmgfRwGLTWjokiRJjzNVdQZw6wTfvj1wdFXdV1VXAlcAzxlvh3kWX0mWT7I88F9JPpJkzSRrJPkQcPIEByRJkvSoOl5kdYUkMwceu01wmHsmuaCdllyu3bYqMLgk17Xttnkar+H+HJoba48FdO8YeK2Aj05woJIkSZPJrKrabD73+TLwCZoa6BPAQcBbF+Tk493bca0FOaAkSdL8muwXO1bVjWPfJ/kq8J/t0+uA1Qbe+uR22zxNZJFVkjwTWJ+BXq+qOnKC45UkSXpcS7JyVV3fPt0BGLsS8kTgO0k+S3Nh4trAb8c71qMWX0n2BraiKb5OBrYDfgVYfEmSpP+zkEm1zleS79LUPiskuRbYG9gqyUY0045X0bZjVdXFSY4FLgEeAPaoqnFvwziR5GtHYEPgd1X1liQzgKMW7ONIkiRNblX1xrlsPnyc9+8H7DfR40+k+Lq3qmYneSDJMsBNPHJuU5IkacFNfP2tKWEixdfMJE8AvkpzBeTdwK+HOipJkqQpaiL3dnxn++1XkpwCLFNVFwx3WJIkaZRkhKKveRZfSTYZ77WqOnc4Q5p8NlhvNU77xcF9D0MaOT+/7Oa+hyCNpLv++kDfQ5jSxku+DhrntQJe/BiPRZIkjaiJ3O9wqhhvkdWtuxyIJEnSKBilQlOSJKl3E1rhXpIkaVjCaDXcm3xJkiR1aCK3FwqwM/CUqto3yerAk6pq3PsWSZIkTdS00Qm+JpR8HQo8Fxhbav8u4EtDG5EkSdIUNpGer82rapMkvwOoqtuSLDLkcUmSpBFi8vVI9yeZTrO2F0lWBGYPdVSSJElT1ESSry8A3wdWSrIfsCPwL0MdlSRJGhnJaF3tOJF7O347yTnANjRXg766qi4d+sgkSZKmoIlc7bg6cA9w0uC2qvrTMAcmSZJGxyj1fE1k2vFHNP1eARYD1gIuA54xxHFJkiRNSROZdnzW4PMkmwDvHNqIJEnSyBmhlq/5X+G+qs4FNh/CWCRJkqa8ifR8vW/g6TRgE+DPQxuRJEkaKQGmjVD0NZGer6UHvn+Apgfs+OEMR5IkaWobt/hqF1dduqo+0NF4JEnSCJrvPqjHsXl+1iQLVdWDwJYdjkeSJGlKGy/5+i1Nf9d5SU4EjgP+MvZiVZ0w5LFJkiRNORPp+VoMuAV4MQ+v91WAxZckSXpMjFC//bjF10rtlY4X8XDRNaaGOipJkqQparziazqwFI8susZYfEmSpMdEEpeaaF1fVft2NhJJkqQRMF7xNTolqCRJ6tUIBV/jLquxTWejkCRJGhHzTL6q6tYuByJJkkbXNJMvSZIkDcNE1vmSJEkamlG7sbbJlyRJUodMviRJUu9GKPgy+ZIkSeqSyZckSepXvNpRkiRJQ2LyJUmSepcRurGOyZckSVKHLL4kSZI65LSjJEnqVbPIat+j6I7JlyRJUodMviRJUu9MviRJkjQUJl+SJKl3GaH7C5l8SZIkdcjkS5Ik9cqrHSVJkjQ0Jl+SJKlfgRFq+TL5kiRJ6pLJlyRJ6t20EYq+TL4kSZI6ZPIlSZJ65dWOkiRJGhqTL0mS1LsRavky+ZIkSeqSxZckSVKHnHaUJEk9C9MYnXlHky9JkqQOmXxJkqReBRvuJUmSNCQmX5IkqV9xkVVJkiQNicmXJEnqnTfWliRJ0lCYfEmSpF55taMkSZKGxuRLkiT1zp4vSZIkDYXJlyRJ6t0IBV8mX5IkSV0y+ZIkSb0Ko5UGjdJnlSRJ6p3FlyRJUoecdpQkSf0KZIQ67k2+JEmSOmTyJUmSejc6uZfJlyRJUqdMviRJUq+CtxeSJEnSkJh8SZKk3o1O7mXyJUmS1CmTL0mS1LsRavky+ZIkSeqSyZckSepZXOFekiRJw2HyJUmSehVGKw0apc8qSZLUO5MvSZLUO3u+JEmSNBQWX5IkSR2y+JIkSb1Lh49HHUvy9SQ3JbloYNvySU5Lcnn7dbl2e5J8IckVSS5IssmjHd/iS5Ik6ZG+CWw7x7aPAKdX1drA6e1zgO2AtdvHbsCXH+3gFl+SJKlfaRruu3o8mqo6A7h1js3bA0e03x8BvHpg+5HVOAt4QpKVxzu+xZckSRo1KySZOfDYbQL7zKiq69vvbwBmtN+vClwz8L5r223z5FITkiSpVz0ssjqrqjZb0J2rqpLUgu5v8iVJkvTobhybTmy/3tRuvw5YbeB9T263zZPFlyRJ6t1k6vmahxOBXdrvdwF+OLD9ze1Vj1sAdwxMT86V046SJEkDknwX2IqmN+xaYG/gAODYJG8DrgZ2at9+MvAK4ArgHuAtj3Z8iy9JktS7yXRzoap64zxe2mYu7y1gj/k5vtOOkiRJHTL5kiRJvRuh+2qbfEmSJHXJ5EuSJPWqWedrdKIvky9JkqQOmXxJkqTe2fMlSZKkobD4kiRJ6pDTjpIkqWchNtxLkiRpGEy+JElS70ap4d7iS1PGn2+8jffu/x1uvvUuEnjT3z2Xt73uRVx8+XV87KDjuO9v9zN9+jT2e++ObLT+Gn0PV5pSTjrlLE77+blQ8NKtN+Hvtt2Cu+6+l4MO+R433Xw7K634BD6w144steTifQ9V6t3Qph2TVJKDBp5/IMnHh3Cej83x/L8f63Po8WH69Gn8yzv/np9+6yP88Cvv4cjvn8kfrrqB/b98Iu/Z9eWc8vUP8v63bsf+Xzmp76FKU8rV19zEaT8/lwP3eTsH7787M3/3B66/4VZOOOlXPGv9tTj0oL141vprccJJv+p7qJqkxhZZ7erRt2H2fN0HvCbJCkM8B8Ajiq+qet6Qz6dJasYKy/KsdVcDYKklFuNpa8zghpvvIAl3/eWvANz1l78yY4Vl+xymNOVc++ebWeepq7Loogszffo0nrHeGpw181J+e85lbP2CDQHY+gUb8puZl/U8UmlyGGbx9QBwGPDeOV9IsmKS45Oc3T62HNh+WpKLk3wtydVjxVuSHyQ5p31tt3bbAcDiSc5L8u12293t16OTvHLgnN9MsmOS6UkObM97QZJ3DPHPQD255vpbufjya9l4/TXYe68d2P/LJ7L5a/fh3w89kQ/v9spHP4CkCVv9yStxyWV/4s677uG+++7nnPOvYNYtd3D7nXez/HJLA7DcE5bi9jvv7nmkmrTS9Hx19ejbsK92/BKwc5I5o4bPAwdX1bOB1wJfa7fvDfy0qp4BfA9YfWCft1bVpsBmwLuSPLGqPgLcW1UbVdXOc5zjGGAngCSLANsAPwLeBtzRnvvZwNuTrDXnwJPslmRmkpm3zJq1wH8A6t5f7rmPd/zrN9h7rx1YesnF+NYPz+Tf9nw1vzl+b/5tz+354KeO7nuI0pSy2qor8ppXbck+nzqKfT99FGutMYNp0x75v5dktJYSkMYz1Ib7qrozyZHAu4B7B156CbB+Hi4/l0myFPB8YId231OS3Dawz7uS7NB+vxqwNnDLOKf/L+DzSRYFtgXOqKp7k7wM2CDJju37lm2PdeUcYz+MJrljo002rfn42OrR/Q88yDv+9Rvs8NJN2e5FGwBw/Clns8+7mv90XrX1Rnz408f0OURpSnrJVpvwkq02AeCoY07nicsvwxOWWYpbb7uL5Zdbmltvu4tll1my51FqMpsMiVRXuljn63M0adPgT900YIs2sdqoqlatqnnm0Um2oinYnltVGwK/AxYb76RV9Vfg58DLgdfTJGHQ9PXtNXDutarqxwv20TSZVBUf/NTRPG2NGbz99Vs9tH3GE5fhrPP+B4Azz72cNZ+8Yk8jlKau2+/4CwA3z7qDs2Zeyguf9yyevck6/OyX5wPws1+ez3M2XbfPIUqTxtCXmqiqW5McS1OAfb3d/GNgL+BAgCQbVdV5wJk0U4WfahOq5dr3LwvcVlX3JFkP2GLgFPcnWbiq7p/L6Y8B/pFmqnLXdtupwD8l+WlV3Z9kHeC6qvrLY/SR1ZOzL7ySE06dyXpPWZlt33ogAB96+ys54EOv5+Nf+D4PPjibRRdZiAM+uFPPI5Wmnk9//ljuuvseFlpoOrvt8gqWXHIxXvN3z+czX/wep//id6y4wrJ8YK/X9T1MTWKjNC3d1TpfBwF7Djx/F/ClJBe0YzgD2B3YB/hukn8Afg3cANwFnALsnuRS4DLgrIFjHQZckOTcufR9/Rj4FvDDqvpbu+1rwJrAuWnmPW8GXv1YfVD15zkbPIU/nXHwXF87+Wvv73g00mjZ/9/e8r+2LbP0Euz7sTf3MBppchta8VVVSw18fyOwxMDzWTRTgXO6A3h5VT2Q5LnAs6vqvva17eZxng8DH57Hee8Hlp/j/bNplqd4xBIVkiSpHwGmjU7wNelWuF8dODbJNOBvwNt7Ho8kSdJjalIVX1V1ObBx3+OQJEndGqWery6udpQkSVLL4kuSJKlDk2raUZIkjSYXWZUkSdJQmHxJkqTe2XAvSZKkoTD5kiRJvRq1RVZNviRJkjpk8iVJknoWe74kSZI0HCZfkiSpX3GdL0mSJA2JyZckSerdCAVfJl+SJEldMvmSJEm9atb5Gp3sy+RLkiSpQyZfkiSpd6OTe5l8SZIkdcriS5IkqUNOO0qSpP6N0LyjyZckSVKHTL4kSVLvvLG2JEmShsLkS5Ik9W6E1lg1+ZIkSeqSyZckSerdCAVfJl+SJEldMvmSJEn9G6Hoy+RLkiSpQyZfkiSpV8F1viRJkjQkJl+SJKlfcZ0vSZIkDYnJlyRJ6t0IBV8mX5IkSV2y+JIkSeqQ046SJKl/IzTvaPIlSZLUIZMvSZLUs7jIqiRJkobD5EuSJPXORVYlSZI0FCZfkiSpV2GkLnY0+ZIkSeqSyZckSerfCEVfJl+SJEkdMvmSJEm9c50vSZIkDYXJlyRJ6p3rfEmSJGkoTL4kSVLvRij4MvmSJEnqksmXJEnq14gtcW/yJUmS1CGLL0mSpA457ShJknrnIquSJEkaCpMvSZLUq+Aiq5IkSRoSky9JktS7EQq+TL4kSZK6ZPIlSZL6N0LRl8mXJElSh0y+JElS71znS5IkSUNh8iVJknrnOl+SJEkaCpMvSZLUuxEKvky+JEmSumTyJUmS+jdC0ZfFlyRJ0oAkVwF3AQ8CD1TVZkmWB44B1gSuAnaqqtsW5PhOO0qSJP1vW1fVRlW1Wfv8I8DpVbU2cHr7fIFYfEmSpF6FZpHVrv5ZQNsDR7TfHwG8ekEPZPElSZJGzQpJZg48dpvj9QJ+nOScgddmVNX17fc3ADMW9OT2fEmSpH6l80VWZw1MJ87N86vquiQrAacl+f3gi1VVSWpBT27yJUmSNKCqrmu/3gR8H3gOcGOSlQHarzct6PEtviRJUu/S4WPccSRLJll67EAO92IAAAzBSURBVHvgZcBFwInALu3bdgF+uKCf1WlHSZKkh80Avp9mHnQh4DtVdUqSs4Fjk7wNuBrYaUFPYPElSZL6N0kWWa2qPwIbzmX7LcA2j8U5nHaUJEnqkMmXJEnq2f9p/a3HHZMvSZKkDpl8SZKk3nW8zlevTL4kSZI6ZPIlSZJ6NZH1t6YSky9JkqQOmXxJkqT+jVD0ZfIlSZLUIYsvSZKkDjntKEmSeuciq5IkSRoKky9JktQ7F1mVJEnSUJh8SZKk3o1Q8GXyJUmS1CWTL0mS1K/Y8yVJkqQhMfmSJEmTwOhEXyZfkiRJHTL5kiRJvQr2fEmSJGlITL4kSVLvRij4MvmSJEnqksnXBJz/u3NnrbTMIlf3PQ4tsBWAWX0PQhpB/uw9fq3R9QlHqefL4msCqmrFvsegBZdkZlVt1vc4pFHjz540d047SpIkdcjkS5Ik9S4j1HJv8qVRcFjfA5BGlD970lyYfGnKqyr/ByD1wJ89zZfRCb5MviRJkrpk8iVJkno3QsGXyZckSVKXTL6kuUgSIFU1u++xSKOq/TmcVlUP9j0WDVfiIqvSyKuqAqrvcUijrP05tPDSlGPxJQFJplXV7IGvGwBvAW4Cjq6qK3seojTlJUlbcI09fzLwbmAJ4HtV9bPeBqehc50vaQQkmdZ+TVtwLdt+3Rz4FHApcCHwgyT+oiINQZLpY99XVSVZNMnSSZYDDgSuBr4FHJLk2X2NU3osWXxpZLR/oe+S5BUAY/1c7V/4TwQuaf9H8AbgM8B1wNuARYGn9DRsacpJ8tYk/www2M+VZFngG8AGwMbAFcBVNCn04sDyY780aQpKh4+e+R+xprQkSyZZrX16L/DTqjo5yUJJDkkyo31tUeB0mr/gFwZ+BGwDHF5V61XVHzofvDRFtGnWegObTqBJl0myZ5LtkixaVXcA6wF/BZYCdgJ2AU6rqqdU1aleBKOpwOJLU06SxZK8tX36DGCH9vulgSWSfApYjaah/kNtAbYiML2q7qYpvP6nqt5XVf/ZHnPnJIt1+kGkqeUQgCQrAncCJyXZEvgTsCPwmvZ9PwfWoEm9LgYOqKrvtfvu2u6vKWiEgi+LL00dA9MR04D72++XB3ZL8nvgg8BKwCLAlsBHgduBvYELgOcnWaiq/gu4MckXknwjyaXApjRNv5LGkcb0gefTq+o+YO0klwHfA5YDzgPeU1UnAv8FvDfJqsCSwJ+r6hLgx8B+Sb6a5Bzg/wHLdvyRpMecTcR63Bu7QnGgh+ueJGcl2Qm4jKbYOr2qPta+fx1gC+C4qvpEkvNoerv+G1ifphB7NfACml6vj1bVDZ1/MOlxYvAqxcHlIZJsCvw+yUrAb4ENq+pF7WufBy5r9/1ekufTJGDbAce2x/pKkv8Gng0c6PT/1DZK63yZfOlxp/3N+qEf07GiK8lLk/xru3kV4NNVdT6wB/BAkie1r11B84vH89rnHwN2pSm2rm2PeWdV/aiqvmjhJc1dkulzWR5imSQHJTmfJl3+CvCnqnodsNLYz2H7c/UHYKxF4HPAMsCTadoAaN93QVUdXlV/mDNVkx6vLL70uJFkWpty1ZxrASX5Dc16QPcmWRz4NXBFkpfSpF8L83CxdSlwA7B1+/xU4H3A26rq1o4+jvS4V1UPtlcLr9hewTgduBu4iGZqf1eaJGuPdpfTgT0HDvEfwPvbY10FfAF4TVUdPXiesV+42h99F13V457FlyatJEu0S0O8fWwtrnYdrpWS/FOSp7dvfSFwPLBjVX2mqu6tqr/R9Iv8v6r6E81aQc9L8kxgHeBK4Pb2CqsHq+q3VXVqH59TmuzmlTgleVWSnwD7AV8Dtm+T6JOAg2l+Bn8FvCDJwjQp2JuSrJXkH4DDgVsGLma5s6p+MOd55vyFS1NROv2nb/Z8adJppxR/QHNF4oXAE4GntVcpbkXTIH8h8OIkvwZOA3YGtkvyO5riag+aguyNSZamWTvo68BRNI33320LNElzkeRJY1Puc/RxLdsuCQGwPfCDqjokyaI0vZMn0FxlvEpVPa/dZxbwwqo6LckPaRZNPYXm/qkvGDunBZZGhcWXJp12GuM24JSq+nKSzWj+Un8VzSXon6yqo9vt3wEObV9fjOYq4s/QNMwfCtwBvLaqvpnkTVV1ew8fSXpcSfJmYIsk+1TVje1ViO+jmUI8rf2l5yc0vZVjfZbvpmmuX57mIpfFkqwJbEIzFfmsJD8DPtJe/Th4vulOJ462YMO9NBl8kuaycmjW+nkSzbIQ2wAXJ1m4qmYCtwHbtr+h303TqLsocFdV3Q+8qS28YuElzV2Sv0+z6PBT203XAfcBG7bPn0NzocqzaNbh+gZwD01z/FIDP1/3Aa8FfkHTd/kTml+E3lxVn2tbB+5rzzl97MIZCy+NGosvTUpVdRnNVOOmNA26S7TrAf0J2KYtrADOBtZIsgLwVeDfaS5TP7I9zp/br05nSPN2G/BOmkWHVwLOok2r2td3BZ4K/BD4APCZqrqHpsh6N7BqW7hdCbyqndLfH9i4qt5cVWfMecKxZv3hfixpcnLaUZPZfjS/ZX+a5i98gG8Ce7QNutNpekv2qapZSXYd6EWRNHFn0iRZf6RpnN+VJnF+bpIlaH7peRHw+qr648B++wFvplkk9Sbg28BHkixZVX+Bhxc/9rZA0sNMvjSZfRe4tqo+UVUXAlTVT2ka5lehWen6PVV1czvtYeElLYC2MDoS+D1wDs36XCsCs2gWJP4ezf8vHgBor0L+PHBrVR0IvLiqtgFm0/Rh3jt4bAsvTUTS3aNvFl+atKrqZmB2kg3hoXW+UlV/rKp3VdWH2kVUnVaU/u++COxZVfvQ3PrnvcATgM2r6hc0fV4HJjkXeB1wclXdn2QR4OVJ/gi8FDjBYksan9OOmuw+CbwYON+/0KXhqapLk2yQ5IlV9e22h+sdwM1JjmmXkzgJuH0wZa6qv7VrfR0351WM0vyYDOtvdcXiS5Pdt021pM4cSLNG3r5VtW+S62jW27sRoKquhof6uDJ2laK34JLmj8WXJjULL6lTR9EsWrwvQFUdPrc3mULrMTdJerG6Ys+XJAl4KMHacHDbWK9lT0OSpiSTL0nSQ9r7p2YsdTblUhfSPkaFyZck6RGc7peGy+RLkiT1b4SiL5MvSZKkDll8SZIkdcjiS9JDkjyY5LwkFyU5rr2v34Ie65tJdmy//1qS9cd571ZJnrcA57iqvan6hLbP8Z675/NcH0/ygUd/p6QFkQ7/6ZvFl6RB91bVRlX1TOBvwO6DLyZZoD7RqvrHqrpknLdsBcx38SVJj0cWX5Lm5ZfA09pU6pdJTgQuSTI9yYFJzk5yQZJ3AKRxSJLL2tvNrDR2oCQ/T7JZ+/22Sc5Ncn6S05OsSVPkvbdN3V6QZMUkx7fnODvJlu2+T0zy4yQXJ/kaE2jRTfKDJOe0++w2x2sHt9tPT7Jiu+2pSU5p9/llkvUeiz9MSeMbpRtre7WjpP+lTbi2A05pN20CPLOqrmwLmDuq6tlJFgXOTPJjYGNgXWB9YAZwCfD1OY67IvBV4IXtsZavqluTfAW4u6o+077vO8DBVfWrJKsDpwJPB/YGftXe+uaVwNsm8HHe2p5jceDsJMdX1S3AksDMqnpvkn9rj70ncBiwe1VdnmRz4FCa+4tK0mPC4kvSoMWTnNd+/0vgcJrpwN9W1ZXt9pcBG4z1cwHLAmsDLwS+297v789JfjqX428BnDF2rKq6dR7jeAmw/sDC6sskWao9x2vafX+U5LYJfKZ3Jdmh/X61dqy3ALOBY9rtRwEntOd4HnDcwLkXncA5JP0fTYJAqjMWX5IG3VtVGw1uaIuQvwxuAvaqqlPneN8rHsNxTAO2qKq/zmUsE5ZkK5pC7rlVdU+SnwOLzePt1Z739jn/DCTpsWTPl6T5dSrwT0kWBkiyTpIlgTOA17c9YSsDW89l37OAFyZZq913+Xb7XcDSA+/7MbDX2JMkY8XQGcCb2m3bAcs9yliXBW5rC6/1aJK3MdOAsfTuTTTTmXcCVyZ5XXuOJHnEvQ4lDUk6fPTM4kvS/PoaTT/XuUkuAv6DJkX/PnB5+9qRwK/n3LGqbgZ2o5niO5+Hp/1OAnYYa7gH3gVs1jb0X8LDV13uQ1O8XUwz/finRxnrKcBCSS4FDqAp/sb8BXhO+xleDOzbbt8ZeFs7vouB7SfwZyJJExZv4SVJkvq0yaab1ZlnzezsfEssknOqarPOTjgHky9JkqQO2XAvSZJ6FSbH+ltdMfmSJEnqkD1fkiSpV0lOAca9H+tjbFZVbdvh+R7B4kuSJKlDTjtKkiR1yOJLkiSpQxZfkiRJHbL4kiRJ6pDFlyRJUof+P3Ud3bgs9VBpAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKq6k9wLBg9u"
      },
      "source": [
        "https://github.com/bentrevett/pytorch-image-classification/blob/master/3_alexnet.ipynb"
      ]
    }
  ]
}